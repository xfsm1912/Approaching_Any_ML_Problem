{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# AlexNet.py\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass AlexNet(nn.Module):\n    def __init__(self):\n        super(AlexNet, self).__init__()\n        # convolution part\n        self.conv1 = nn.Conv2d(\n            in_channels=3,\n            out_channels=96,\n            kernel_size=11,\n            stride=4,\n            padding=0\n        )\n        self.pool1 = nn.MaxPool2d(kernel_size=3, stride=2)\n        self.conv2 = nn.Conv2d(\n            in_channels=96,\n            out_channels=256,\n            kernel_size=5,\n            stride=1,\n            padding=2\n        )\n        self.pool2 = nn.MaxPool2d(kernel_size=3, stride=2)\n        self.conv3 = nn.Conv2d(\n            in_channels=256,\n            out_channels=384,\n            kernel_size=3,\n            stride=1,\n            padding=1\n        )\n        self.conv4 = nn.Conv2d(\n            in_channels=384,\n            out_channels=384,\n            kernel_size=3,\n            stride=1,\n            padding=1\n        )\n        self.conv5 = nn.Conv2d(\n            in_channels=384,\n            out_channels=256,\n            kernel_size=3,\n            stride=1,\n            padding=1\n        )\n        self.pool3 = nn.MaxPool2d(kernel_size=3, stride=2)\n        # dense part\n        self.fc1 = nn.Linear(\n            in_features=9216,\n            out_features=4096\n        )\n        self.dropout1 = nn.Dropout(0.5)\n        self.fc2 = nn.Linear(\n            in_features=4096,\n            out_features=4096\n        )\n        self.dropout2 = nn.Dropout(0.5)\n        self.fc3 = nn.Linear(\n            in_features=4096,\n            out_features=1000\n        )\n\n    def forward(self, image):\n        # get the batch size, channels, height and width\n        # of the input batch of images\n        # original size: (bs, 3, 227, 227)\n        bs, c, h, w = image.size()\n        x = F.relu(self.conv1(image))  # size: (bs, 96, 55, 55)\n        x = self.pool1(x)  # size: (bs, 96, 27, 27)\n        x = F.relu(self.conv2(x))  # size: (bs, 256, 27, 27)\n        x = self.pool2(x)  # size: (bs, 256, 13, 13)\n        x = F.relu(self.conv3(x))  # size: (bs, 384, 13, 13)\n        x = F.relu(self.conv4(x))  # size: (bs, 384, 13, 13)\n        x = F.relu(self.conv5(x))  # size: (bs, 256, 13, 13)\n        x = self.pool3(x)  # size: (bs, 256, 6, 6)\n        x = x.view(bs, -1)  # size: (bs, 9216)\n        x = F.relu(self.fc1(x))  # size: (bs, 4096)\n        x = self.dropout1(x)  # size: (bs, 4096)\n        # dropout does not change size\n        # dropout is used for regularization\n        # 0.3 dropout means that only 70% of the nodes\n        # of the current layer arre used for the next layer\n        x = F.relu(self.fc2(x))  # size: (bs, 4096)\n        x = self.dropout2(x)  # size: (bs, 1000)\n        # 1000 is number of classes in ImageNet Dataset\n        # softmax is an activation function that converts\n        # linear output to probabilities that add up to 1\n        # for each sample in the batch\n        x = torch.softmax(x, axis=1)  # size: (bs, 1000)\n        return x\n","metadata":{"execution":{"iopub.status.busy":"2021-05-28T23:09:37.254983Z","iopub.execute_input":"2021-05-28T23:09:37.255383Z","iopub.status.idle":"2021-05-28T23:09:38.522534Z","shell.execute_reply.started":"2021-05-28T23:09:37.255304Z","shell.execute_reply":"2021-05-28T23:09:38.521819Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# dataset.py\n# import torch\n# import numpy as np\nfrom PIL import Image\nfrom PIL import ImageFile\n\n# sometimes, you will have images without an ending bit\n# this takes care of those kind of (corrupt) images\nImageFile.LOAD_TRUNCATED_IMAGES = True\n\n\nclass ClassificationDataset:\n    \"\"\"\n    A general classification dataset class that you can use for all\n    kinds of image classification problems. For example,\n    binary classification, multi-class, multi-label classification\n    \"\"\"\n    def __init__(self, image_paths, targets, resize=None, augmentations=None):\n        \"\"\"\n\n        :param image_paths: list of path to images\n        :param targets: numpy array\n        :param resize: tuple, e.g. (256, 256), resizes image if not None\n        :param augmentations: albumentation augmentations\n        \"\"\"\n        self.image_paths = image_paths\n        self.targets = targets\n        self.resize = resize\n        self.augmentations = augmentations\n\n    def __len__(self):\n        \"\"\"\n        Return the total number of samples in the dataset\n        :return:\n        \"\"\"\n        return len(self.image_paths)\n\n    def __getitem__(self, item):\n        \"\"\"\n        For a given 'item' index, return everything we need\n        to train given model\n        :param item:\n        :return:\n        \"\"\"\n        # use PIL to open the image\n        image = Image.open(self.image_paths[item])\n        # convert image to RGB, we have single channel images\n        image = image.convert('RGB')\n        # grab correct target\n        targets = self.targets[item]\n\n        # resize if needed\n        if self.resize is not None:\n            image = image.resize(\n                (self.resize[1], self.resize[0]),\n                resample=Image.BILINEAR\n            )\n\n        # convert image to numpy array\n        image = np.array(image)\n\n        # if we have albumentation augmentations\n        # add them to the image\n        if self.augmentations is not None:\n            augmented = self.augmentations(image=image)\n            image = augmented['image']\n\n        # pytorch expects CHW instead of HWC\n        image = np.transpose(image, (2, 0, 1)).astype(np.float32)\n\n        # return tensors of image and targets\n        # take a look at the types!\n        # for regression tasks,\n        return {\n            'image': torch.tensor(image, dtype=torch.float),\n            'targets': torch.tensor(targets, dtype=torch.long)\n        }\n","metadata":{"execution":{"iopub.status.busy":"2021-05-28T23:10:00.213860Z","iopub.execute_input":"2021-05-28T23:10:00.214176Z","iopub.status.idle":"2021-05-28T23:10:00.224818Z","shell.execute_reply.started":"2021-05-28T23:10:00.214145Z","shell.execute_reply":"2021-05-28T23:10:00.223778Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# engine.py\n# import torch\nimport torch.nn as nn\n\nfrom tqdm import tqdm\n\n\ndef train(data_loader, model, optimizer, device):\n    \"\"\"\n    This function does training for one epoch\n    :param data_loader: this is the pytorch dataloader\n    :param model: pytorch model\n    :param optimizer: optimizer, for e.g. adam, sgd, etc\n    :param device: cuda/cpu\n    :return:\n    \"\"\"\n\n    # put the model in train mode\n    model.train()\n\n    # go over every batch of data in data loader\n    for data in data_loader:\n        # remember, we have image and targets\n        # in our dataset class\n        inputs = data['image']\n        targets = data['targets']\n\n        # move inputs/targets to cuda/cpu device\n        inputs = inputs.to(device, dtype=torch.float)\n        targets = targets.to(device, dtype=torch.float)\n\n        # zero grad the optimizer\n        optimizer.zero_grad()\n        # do the forward step of model\n        outputs = model(inputs)\n        # calculate loss\n        loss = nn.BCEWithLogitsLoss()(outputs, targets.view(-1, 1))\n        # backward tep the loss\n        loss.backward()\n        # step optimizer\n        optimizer.step()\n        # if you have a scheduler, you either need to\n        # step it here or you have to step it after\n        # the epoch. here, we are not using any learning\n        # rate scheduler\n\n\ndef evaluate(data_loader, model, device):\n    \"\"\"\n    This function does evaluation for one epoch\n    :param data_loader: this is the pytorch dataloader\n    :param model: pytorch model\n    :param device: cuda/cpu\n    :return:\n    \"\"\"\n    # put model in evaluation mode\n    model.eval()\n\n    # init lists to store targets and outputs\n    final_targets = []\n    final_outputs = []\n\n    # we use no_grad context\n    with torch.no_grad():\n\n        for data in data_loader:\n            inputs = data['image']\n            targets = data['targets']\n            inputs = inputs.to(device, dtype=torch.float)\n            targets = targets.to(device, dtype=torch.float)\n\n            # do the forward step to generate prediction\n            output = model(inputs)\n\n            # convert targets and outputs to lists\n            targets = targets.detach().cpu().numpy().tolist()\n            output = output.detach().cpu().numpy().tolist()\n\n            # extend the original list\n            final_targets.extend(targets)\n            final_outputs.extend(output)\n\n    # return final output and final targets\n    return final_outputs, final_targets","metadata":{"execution":{"iopub.status.busy":"2021-05-28T23:31:04.916069Z","iopub.execute_input":"2021-05-28T23:31:04.916381Z","iopub.status.idle":"2021-05-28T23:31:04.928901Z","shell.execute_reply.started":"2021-05-28T23:31:04.916352Z","shell.execute_reply":"2021-05-28T23:31:04.926275Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"!pip install pretrainedmodels","metadata":{"execution":{"iopub.status.busy":"2021-05-28T23:11:42.525291Z","iopub.execute_input":"2021-05-28T23:11:42.525610Z","iopub.status.idle":"2021-05-28T23:11:51.268594Z","shell.execute_reply.started":"2021-05-28T23:11:42.525580Z","shell.execute_reply":"2021-05-28T23:11:51.267762Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Collecting pretrainedmodels\n  Downloading pretrainedmodels-0.7.4.tar.gz (58 kB)\n\u001b[K     |████████████████████████████████| 58 kB 872 kB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (from pretrainedmodels) (1.7.0)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.7/site-packages (from pretrainedmodels) (0.8.1)\nRequirement already satisfied: munch in /opt/conda/lib/python3.7/site-packages (from pretrainedmodels) (2.5.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from pretrainedmodels) (4.56.2)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from munch->pretrainedmodels) (1.15.0)\nRequirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from torch->pretrainedmodels) (0.18.2)\nRequirement already satisfied: typing_extensions in /opt/conda/lib/python3.7/site-packages (from torch->pretrainedmodels) (3.7.4.3)\nRequirement already satisfied: dataclasses in /opt/conda/lib/python3.7/site-packages (from torch->pretrainedmodels) (0.6)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torch->pretrainedmodels) (1.19.5)\nRequirement already satisfied: pillow>=4.1.1 in /opt/conda/lib/python3.7/site-packages (from torchvision->pretrainedmodels) (7.2.0)\nBuilding wheels for collected packages: pretrainedmodels\n  Building wheel for pretrainedmodels (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pretrainedmodels: filename=pretrainedmodels-0.7.4-py3-none-any.whl size=60963 sha256=6c9975427f4d3122c696626121f5d6997db00891369ef4f7520a2323779bef2d\n  Stored in directory: /root/.cache/pip/wheels/ed/27/e8/9543d42de2740d3544db96aefef63bda3f2c1761b3334f4873\nSuccessfully built pretrainedmodels\nInstalling collected packages: pretrainedmodels\nSuccessfully installed pretrainedmodels-0.7.4\n","output_type":"stream"}]},{"cell_type":"code","source":"# model.py\n# import torch.nn as nn\nimport pretrainedmodels\n\n\ndef get_model(pretrained):\n    if pretrained:\n        model = pretrainedmodels.__dict__['alexnet'](\n            pretrained='imagenet'\n        )\n    else:\n        model = pretrainedmodels.__dict__['alexnet'](\n            pretrained=None\n        )\n    # print the model here to know what's going on\n    model.last_linear = nn.Sequential(\n        nn.BatchNorm1d(4096),\n        nn.Dropout(p=0.25),\n        nn.Linear(in_features=4096, out_features=2048),\n        nn.ReLU(),\n        nn.BatchNorm1d(2048, eps=1e-05, momentum=0.1),\n        nn.Dropout(p=0.5),\n        nn.Linear(in_features=2048, out_features=1)\n    )\n    return model\n\n\ndef get_resnet18_model(pretrained):\n    if pretrained:\n        model = pretrainedmodels.__dict__['resnet18'](\n            pretrained='imagenet'\n        )\n    else:\n        model = pretrainedmodels.__dict__['resnet18'](\n            pretrained=None\n        )\n    # print the model here to know what's going on\n    model.last_linear = nn.Sequential(\n        nn.BatchNorm1d(512),\n        nn.Dropout(p=0.25),\n        nn.Linear(in_features=512, out_features=2048),\n        nn.ReLU(),\n        nn.BatchNorm1d(2048, eps=1e-05, momentum=0.1),\n        nn.Dropout(p=0.5),\n        nn.Linear(in_features=2048, out_features=1)\n    )\n    return model","metadata":{"execution":{"iopub.status.busy":"2021-05-28T23:11:52.253867Z","iopub.execute_input":"2021-05-28T23:11:52.254221Z","iopub.status.idle":"2021-05-28T23:11:53.511024Z","shell.execute_reply.started":"2021-05-28T23:11:52.254190Z","shell.execute_reply":"2021-05-28T23:11:53.510268Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# train.py\nimport os\n\nimport pandas as pd\nimport numpy as np\n\nimport albumentations\nimport torch\n\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split\n\n# location of train.csv and train_png folder\n# with all the png images\ndata_path = \"../input/siim-png-train-csv/\"\n\n# cuda/cpu device\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# let's train for 10 epoches\nepoches = 10\n\n# load the dataframe\n# https://www.kaggle.com/abhishek/train-your-own-mask-rcnn/data?select=train-rle.csv\ndf = pd.read_csv(os.path.join(data_path, 'train.csv'))\n\n# fetch all image ids\nimages = df.ImageId.values.tolist()\n\n# a list with image locations\n# https://www.kaggle.com/abhishek/siim-png-images\nimages = [\n    os.path.join(\"../input/siim-png-images/\", 'train_png', image + '.png') for image in images\n]\n\n# binary target numpy array\ntargets = df.target.values\n\n# fetch out model, we will try both pretrained\n# and non-pretrained weights\nmodel = get_model(pretrained=True)\n\n# move model to device\nmodel.to(device)\n\n# mean and std values of RGB channels for imagenet dataset\n# we use these pre-calculated values when we use weights\n# from imagenet\n# when we do not use imagenet weights, we use the mean and\n# standard deviation values of the original dataset\n# please note that this is a separate calculation\nmean = (0.485, 0.456, 0.406)\nstd = (0.229, 0.224, 0.225)\n\n# albumentation is an image augmentation library\n# that allows you to do many different types of image\n# augmentations. here, i am using only normalization\n# notice always_apply=True. we always want to apply\n# normalization\naug = albumentations.Compose(\n    [\n        albumentations.Normalize(\n            mean, std, max_pixel_value=255.0, always_apply=True\n        )\n    ]\n)\n\n# instead of using kfold, i am using train_test_split\n# with a fixed random state\ntrain_images, valid_images, train_targets, valid_targets = train_test_split(\n    images, targets, stratify=targets, random_state=42\n)\n\n# fetch the ClassificationDataset class\ntrain_dataset = ClassificationDataset(\n    image_paths=train_images,\n    targets=train_targets,\n    resize=(227, 227),\n    augmentations=aug\n)\n\n# torch dataloader creates batches of data\n# from classification dataset class\ntrain_loader = torch.utils.data.DataLoader(\n    train_dataset,\n    batch_size=16,\n    shuffle=True,\n    num_workers=4\n)\n\n# same for validation\nvalid_dataset = ClassificationDataset(\n    image_paths=valid_images,\n    targets=valid_targets,\n    resize=(227, 227),\n    augmentations=aug\n)\n\nvalid_loader = torch.utils.data.DataLoader(\n    valid_dataset,\n    batch_size=16,\n    shuffle=False,\n    num_workers=4\n)\n\n# simple Adam optimizer\noptimizer = torch.optim.Adam(model.parameters(), lr=5e-4)\n\n# train and print auc score for all epoches\nfor epoch in range(epoches):\n    train(train_loader, model, optimizer, device=device)\n    predictions, valid_targets = evaluate(\n        valid_loader, model, device=device\n    )\n    roc_auc = metrics.roc_auc_score(valid_targets, predictions)\n    print(\n        f'Epoch={epoch}, Valid ROC AUC={roc_auc}'\n    )","metadata":{"execution":{"iopub.status.busy":"2021-05-28T23:51:32.427591Z","iopub.execute_input":"2021-05-28T23:51:32.427939Z","iopub.status.idle":"2021-05-29T00:19:50.231188Z","shell.execute_reply.started":"2021-05-28T23:51:32.427908Z","shell.execute_reply":"2021-05-29T00:19:50.230012Z"},"trusted":true},"execution_count":39,"outputs":[{"name":"stdout","text":"Epoch=0, Valid ROC AUC=0.57624287902239\nEpoch=1, Valid ROC AUC=0.511279304392924\nEpoch=2, Valid ROC AUC=0.5893466933542945\nEpoch=3, Valid ROC AUC=0.6132119964668605\nEpoch=4, Valid ROC AUC=0.5969652277497306\nEpoch=5, Valid ROC AUC=0.585839080087194\nEpoch=6, Valid ROC AUC=0.6002795718094374\nEpoch=7, Valid ROC AUC=0.672852361774025\n","output_type":"stream"},{"name":"stderr","text":"Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f063634e680>\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1203, in __del__\n    self._shutdown_workers()\n  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1177, in _shutdown_workers\n    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 138, in join\n    assert self._parent_pid == os.getpid(), 'can only join a child process'\nAssertionError: can only join a child process\nException ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f063634e680>\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1203, in __del__\n    self._shutdown_workers()\n  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1177, in _shutdown_workers\n    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 138, in join\n    assert self._parent_pid == os.getpid(), 'can only join a child process'\nAssertionError: can only join a child process\nException ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f063634e680>\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1203, in __del__\n    self._shutdown_workers()\n  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1177, in _shutdown_workers\n    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 138, in join\n    assert self._parent_pid == os.getpid(), 'can only join a child process'\nAssertionError: can only join a child process\nException ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f063634e680>\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1203, in __del__\n    self._shutdown_workers()\n  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1177, in _shutdown_workers\n    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 138, in join\n    assert self._parent_pid == os.getpid(), 'can only join a child process'\nAssertionError: can only join a child process\nException ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f063634e680>\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1203, in __del__\n    self._shutdown_workers()\n  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1177, in _shutdown_workers\n    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 138, in join\n    assert self._parent_pid == os.getpid(), 'can only join a child process'\nAssertionError: can only join a child process\nException ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f063634e680>\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1203, in __del__\n    self._shutdown_workers()\nException ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f063634e680>\n  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1177, in _shutdown_workers\nTraceback (most recent call last):\n    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\nException ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f063634e680>\n  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 138, in join\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1203, in __del__\n    self._shutdown_workers()\n  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1203, in __del__\n    assert self._parent_pid == os.getpid(), 'can only join a child process'\n    self._shutdown_workers()\n  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1177, in _shutdown_workers\n  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1177, in _shutdown_workers\n    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\nAssertionError: can only join a child process\n  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 138, in join\n    assert self._parent_pid == os.getpid(), 'can only join a child process'\n  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 138, in join\nAssertionError: can only join a child process\n    assert self._parent_pid == os.getpid(), 'can only join a child process'\nAssertionError: can only join a child process\n","output_type":"stream"},{"name":"stdout","text":"Epoch=8, Valid ROC AUC=0.5801560740014424\nEpoch=9, Valid ROC AUC=0.6138327269191186\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}