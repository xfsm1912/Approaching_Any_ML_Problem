{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "useful-richards",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-05-29T15:34:09.618706Z",
     "iopub.status.busy": "2021-05-29T15:34:09.617560Z",
     "iopub.status.idle": "2021-05-29T15:34:09.620255Z",
     "shell.execute_reply": "2021-05-29T15:34:09.619801Z",
     "shell.execute_reply.started": "2021-05-29T15:31:20.527165Z"
    },
    "papermill": {
     "duration": 0.020877,
     "end_time": "2021-05-29T15:34:09.620389",
     "exception": false,
     "start_time": "2021-05-29T15:34:09.599512",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "# for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "#     for filename in filenames:\n",
    "#         print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "vietnamese-message",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-29T15:34:09.639356Z",
     "iopub.status.busy": "2021-05-29T15:34:09.638734Z",
     "iopub.status.idle": "2021-05-29T15:34:10.773348Z",
     "shell.execute_reply": "2021-05-29T15:34:10.772719Z",
     "shell.execute_reply.started": "2021-05-29T15:32:29.739860Z"
    },
    "papermill": {
     "duration": 1.145181,
     "end_time": "2021-05-29T15:34:10.773489",
     "exception": false,
     "start_time": "2021-05-29T15:34:09.628308",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "specialized-doctrine",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-29T15:34:10.804322Z",
     "iopub.status.busy": "2021-05-29T15:34:10.798148Z",
     "iopub.status.idle": "2021-05-29T15:34:10.806746Z",
     "shell.execute_reply": "2021-05-29T15:34:10.806320Z",
     "shell.execute_reply.started": "2021-05-28T23:09:37.255304Z"
    },
    "papermill": {
     "duration": 0.024131,
     "end_time": "2021-05-29T15:34:10.806849",
     "exception": false,
     "start_time": "2021-05-29T15:34:10.782718",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# AlexNet.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class AlexNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AlexNet, self).__init__()\n",
    "        # convolution part\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=3,\n",
    "            out_channels=96,\n",
    "            kernel_size=11,\n",
    "            stride=4,\n",
    "            padding=0\n",
    "        )\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=96,\n",
    "            out_channels=256,\n",
    "            kernel_size=5,\n",
    "            stride=1,\n",
    "            padding=2\n",
    "        )\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        self.conv3 = nn.Conv2d(\n",
    "            in_channels=256,\n",
    "            out_channels=384,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding=1\n",
    "        )\n",
    "        self.conv4 = nn.Conv2d(\n",
    "            in_channels=384,\n",
    "            out_channels=384,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding=1\n",
    "        )\n",
    "        self.conv5 = nn.Conv2d(\n",
    "            in_channels=384,\n",
    "            out_channels=256,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding=1\n",
    "        )\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        # dense part\n",
    "        self.fc1 = nn.Linear(\n",
    "            in_features=9216,\n",
    "            out_features=4096\n",
    "        )\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(\n",
    "            in_features=4096,\n",
    "            out_features=4096\n",
    "        )\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc3 = nn.Linear(\n",
    "            in_features=4096,\n",
    "            out_features=1000\n",
    "        )\n",
    "\n",
    "    def forward(self, image):\n",
    "        # get the batch size, channels, height and width\n",
    "        # of the input batch of images\n",
    "        # original size: (bs, 3, 227, 227)\n",
    "        bs, c, h, w = image.size()\n",
    "        x = F.relu(self.conv1(image))  # size: (bs, 96, 55, 55)\n",
    "        x = self.pool1(x)  # size: (bs, 96, 27, 27)\n",
    "        x = F.relu(self.conv2(x))  # size: (bs, 256, 27, 27)\n",
    "        x = self.pool2(x)  # size: (bs, 256, 13, 13)\n",
    "        x = F.relu(self.conv3(x))  # size: (bs, 384, 13, 13)\n",
    "        x = F.relu(self.conv4(x))  # size: (bs, 384, 13, 13)\n",
    "        x = F.relu(self.conv5(x))  # size: (bs, 256, 13, 13)\n",
    "        x = self.pool3(x)  # size: (bs, 256, 6, 6)\n",
    "        x = x.view(bs, -1)  # size: (bs, 9216)\n",
    "        x = F.relu(self.fc1(x))  # size: (bs, 4096)\n",
    "        x = self.dropout1(x)  # size: (bs, 4096)\n",
    "        # dropout does not change size\n",
    "        # dropout is used for regularization\n",
    "        # 0.3 dropout means that only 70% of the nodes\n",
    "        # of the current layer arre used for the next layer\n",
    "        x = F.relu(self.fc2(x))  # size: (bs, 4096)\n",
    "        x = self.dropout2(x)  # size: (bs, 1000)\n",
    "        # 1000 is number of classes in ImageNet Dataset\n",
    "        # softmax is an activation function that converts\n",
    "        # linear output to probabilities that add up to 1\n",
    "        # for each sample in the batch\n",
    "        x = torch.softmax(x, axis=1)  # size: (bs, 1000)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dynamic-spring",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-29T15:34:10.832234Z",
     "iopub.status.busy": "2021-05-29T15:34:10.831488Z",
     "iopub.status.idle": "2021-05-29T15:34:10.834028Z",
     "shell.execute_reply": "2021-05-29T15:34:10.833642Z",
     "shell.execute_reply.started": "2021-05-28T23:10:00.214145Z"
    },
    "papermill": {
     "duration": 0.019387,
     "end_time": "2021-05-29T15:34:10.834127",
     "exception": false,
     "start_time": "2021-05-29T15:34:10.814740",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dataset.py\n",
    "# import torch\n",
    "# import numpy as np\n",
    "from PIL import Image\n",
    "from PIL import ImageFile\n",
    "\n",
    "# sometimes, you will have images without an ending bit\n",
    "# this takes care of those kind of (corrupt) images\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "\n",
    "class ClassificationDataset:\n",
    "    \"\"\"\n",
    "    A general classification dataset class that you can use for all\n",
    "    kinds of image classification problems. For example,\n",
    "    binary classification, multi-class, multi-label classification\n",
    "    \"\"\"\n",
    "    def __init__(self, image_paths, targets, resize=None, augmentations=None):\n",
    "        \"\"\"\n",
    "\n",
    "        :param image_paths: list of path to images\n",
    "        :param targets: numpy array\n",
    "        :param resize: tuple, e.g. (256, 256), resizes image if not None\n",
    "        :param augmentations: albumentation augmentations\n",
    "        \"\"\"\n",
    "        self.image_paths = image_paths\n",
    "        self.targets = targets\n",
    "        self.resize = resize\n",
    "        self.augmentations = augmentations\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Return the total number of samples in the dataset\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        \"\"\"\n",
    "        For a given 'item' index, return everything we need\n",
    "        to train given model\n",
    "        :param item:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # use PIL to open the image\n",
    "        image = Image.open(self.image_paths[item])\n",
    "        # convert image to RGB, we have single channel images\n",
    "        image = image.convert('RGB')\n",
    "        # grab correct target\n",
    "        targets = self.targets[item]\n",
    "\n",
    "        # resize if needed\n",
    "        if self.resize is not None:\n",
    "            image = image.resize(\n",
    "                (self.resize[1], self.resize[0]),\n",
    "                resample=Image.BILINEAR\n",
    "            )\n",
    "\n",
    "        # convert image to numpy array\n",
    "        image = np.array(image)\n",
    "\n",
    "        # if we have albumentation augmentations\n",
    "        # add them to the image\n",
    "        if self.augmentations is not None:\n",
    "            augmented = self.augmentations(image=image)\n",
    "            image = augmented['image']\n",
    "\n",
    "        # pytorch expects CHW instead of HWC\n",
    "        image = np.transpose(image, (2, 0, 1)).astype(np.float32)\n",
    "\n",
    "        # return tensors of image and targets\n",
    "        # take a look at the types!\n",
    "        # for regression tasks,\n",
    "        return {\n",
    "            'image': torch.tensor(image, dtype=torch.float),\n",
    "            'targets': torch.tensor(targets, dtype=torch.long)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "color-principal",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-29T15:34:10.859717Z",
     "iopub.status.busy": "2021-05-29T15:34:10.859009Z",
     "iopub.status.idle": "2021-05-29T15:34:10.861723Z",
     "shell.execute_reply": "2021-05-29T15:34:10.861317Z",
     "shell.execute_reply.started": "2021-05-28T23:31:04.916352Z"
    },
    "papermill": {
     "duration": 0.019649,
     "end_time": "2021-05-29T15:34:10.861823",
     "exception": false,
     "start_time": "2021-05-29T15:34:10.842174",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# engine.py\n",
    "# import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def train(data_loader, model, optimizer, device):\n",
    "    \"\"\"\n",
    "    This function does training for one epoch\n",
    "    :param data_loader: this is the pytorch dataloader\n",
    "    :param model: pytorch model\n",
    "    :param optimizer: optimizer, for e.g. adam, sgd, etc\n",
    "    :param device: cuda/cpu\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    # put the model in train mode\n",
    "    model.train()\n",
    "\n",
    "    # go over every batch of data in data loader\n",
    "    for data in data_loader:\n",
    "        # remember, we have image and targets\n",
    "        # in our dataset class\n",
    "        inputs = data['image']\n",
    "        targets = data['targets']\n",
    "\n",
    "        # move inputs/targets to cuda/cpu device\n",
    "        inputs = inputs.to(device, dtype=torch.float)\n",
    "        targets = targets.to(device, dtype=torch.float)\n",
    "\n",
    "        # zero grad the optimizer\n",
    "        optimizer.zero_grad()\n",
    "        # do the forward step of model\n",
    "        outputs = model(inputs)\n",
    "        # calculate loss\n",
    "        loss = nn.BCEWithLogitsLoss()(outputs, targets.view(-1, 1))\n",
    "        # backward tep the loss\n",
    "        loss.backward()\n",
    "        # step optimizer\n",
    "        optimizer.step()\n",
    "        # if you have a scheduler, you either need to\n",
    "        # step it here or you have to step it after\n",
    "        # the epoch. here, we are not using any learning\n",
    "        # rate scheduler\n",
    "\n",
    "\n",
    "def evaluate(data_loader, model, device):\n",
    "    \"\"\"\n",
    "    This function does evaluation for one epoch\n",
    "    :param data_loader: this is the pytorch dataloader\n",
    "    :param model: pytorch model\n",
    "    :param device: cuda/cpu\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # put model in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # init lists to store targets and outputs\n",
    "    final_targets = []\n",
    "    final_outputs = []\n",
    "\n",
    "    # we use no_grad context\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for data in data_loader:\n",
    "            inputs = data['image']\n",
    "            targets = data['targets']\n",
    "            inputs = inputs.to(device, dtype=torch.float)\n",
    "            targets = targets.to(device, dtype=torch.float)\n",
    "\n",
    "            # do the forward step to generate prediction\n",
    "            output = model(inputs)\n",
    "\n",
    "            # convert targets and outputs to lists\n",
    "            targets = targets.detach().cpu().numpy().tolist()\n",
    "            output = output.detach().cpu().numpy().tolist()\n",
    "\n",
    "            # extend the original list\n",
    "            final_targets.extend(targets)\n",
    "            final_outputs.extend(output)\n",
    "\n",
    "    # return final output and final targets\n",
    "    return final_outputs, final_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "universal-evolution",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-29T15:34:10.881220Z",
     "iopub.status.busy": "2021-05-29T15:34:10.880687Z",
     "iopub.status.idle": "2021-05-29T15:34:20.022480Z",
     "shell.execute_reply": "2021-05-29T15:34:20.021579Z",
     "shell.execute_reply.started": "2021-05-28T23:11:42.52558Z"
    },
    "papermill": {
     "duration": 9.152924,
     "end_time": "2021-05-29T15:34:20.022651",
     "exception": false,
     "start_time": "2021-05-29T15:34:10.869727",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pretrainedmodels\r\n",
      "  Downloading pretrainedmodels-0.7.4.tar.gz (58 kB)\r\n",
      "\u001b[K     |████████████████████████████████| 58 kB 614 kB/s \r\n",
      "\u001b[?25hRequirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (from pretrainedmodels) (1.7.0)\r\n",
      "Requirement already satisfied: torchvision in /opt/conda/lib/python3.7/site-packages (from pretrainedmodels) (0.8.1)\r\n",
      "Requirement already satisfied: munch in /opt/conda/lib/python3.7/site-packages (from pretrainedmodels) (2.5.0)\r\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from pretrainedmodels) (4.59.0)\r\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from munch->pretrainedmodels) (1.15.0)\r\n",
      "Requirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from torch->pretrainedmodels) (0.18.2)\r\n",
      "Requirement already satisfied: typing_extensions in /opt/conda/lib/python3.7/site-packages (from torch->pretrainedmodels) (3.7.4.3)\r\n",
      "Requirement already satisfied: dataclasses in /opt/conda/lib/python3.7/site-packages (from torch->pretrainedmodels) (0.6)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torch->pretrainedmodels) (1.19.5)\r\n",
      "Requirement already satisfied: pillow>=4.1.1 in /opt/conda/lib/python3.7/site-packages (from torchvision->pretrainedmodels) (7.2.0)\r\n",
      "Building wheels for collected packages: pretrainedmodels\r\n",
      "  Building wheel for pretrainedmodels (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Created wheel for pretrainedmodels: filename=pretrainedmodels-0.7.4-py3-none-any.whl size=60963 sha256=98bbc5994423e8217ac0383aa4f399e87e64ccc49b00938c30557a8d964555e4\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/ed/27/e8/9543d42de2740d3544db96aefef63bda3f2c1761b3334f4873\r\n",
      "Successfully built pretrainedmodels\r\n",
      "Installing collected packages: pretrainedmodels\r\n",
      "Successfully installed pretrainedmodels-0.7.4\r\n"
     ]
    }
   ],
   "source": [
    "!pip install pretrainedmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "exposed-portuguese",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-29T15:34:20.054337Z",
     "iopub.status.busy": "2021-05-29T15:34:20.053593Z",
     "iopub.status.idle": "2021-05-29T15:34:21.226453Z",
     "shell.execute_reply": "2021-05-29T15:34:21.225972Z",
     "shell.execute_reply.started": "2021-05-28T23:11:52.25419Z"
    },
    "papermill": {
     "duration": 1.192356,
     "end_time": "2021-05-29T15:34:21.226615",
     "exception": false,
     "start_time": "2021-05-29T15:34:20.034259",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model.py\n",
    "# import torch.nn as nn\n",
    "import pretrainedmodels\n",
    "\n",
    "\n",
    "def get_model(pretrained):\n",
    "    if pretrained:\n",
    "        model = pretrainedmodels.__dict__['alexnet'](\n",
    "            pretrained='imagenet'\n",
    "        )\n",
    "    else:\n",
    "        model = pretrainedmodels.__dict__['alexnet'](\n",
    "            pretrained=None\n",
    "        )\n",
    "    # print the model here to know what's going on\n",
    "    model.last_linear = nn.Sequential(\n",
    "        nn.BatchNorm1d(4096),\n",
    "        nn.Dropout(p=0.25),\n",
    "        nn.Linear(in_features=4096, out_features=2048),\n",
    "        nn.ReLU(),\n",
    "        nn.BatchNorm1d(2048, eps=1e-05, momentum=0.1),\n",
    "        nn.Dropout(p=0.5),\n",
    "        nn.Linear(in_features=2048, out_features=1)\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_resnet18_model(pretrained):\n",
    "    if pretrained:\n",
    "        model = pretrainedmodels.__dict__['resnet18'](\n",
    "            pretrained='imagenet'\n",
    "        )\n",
    "    else:\n",
    "        model = pretrainedmodels.__dict__['resnet18'](\n",
    "            pretrained=None\n",
    "        )\n",
    "    # print the model here to know what's going on\n",
    "    model.last_linear = nn.Sequential(\n",
    "        nn.BatchNorm1d(512),\n",
    "        nn.Dropout(p=0.25),\n",
    "        nn.Linear(in_features=512, out_features=2048),\n",
    "        nn.ReLU(),\n",
    "        nn.BatchNorm1d(2048, eps=1e-05, momentum=0.1),\n",
    "        nn.Dropout(p=0.5),\n",
    "        nn.Linear(in_features=2048, out_features=1)\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "addressed-compression",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-29T15:34:21.260481Z",
     "iopub.status.busy": "2021-05-29T15:34:21.259954Z",
     "iopub.status.idle": "2021-05-29T16:02:43.268277Z",
     "shell.execute_reply": "2021-05-29T16:02:43.268723Z",
     "shell.execute_reply.started": "2021-05-28T23:51:32.427908Z"
    },
    "papermill": {
     "duration": 1702.030975,
     "end_time": "2021-05-29T16:02:43.268892",
     "exception": false,
     "start_time": "2021-05-29T15:34:21.237917",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-5c106cde.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64fd8f2b098341dfb480eea1b3144046",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/44.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch=0, Valid ROC AUC=0.7957189047267896\n",
      "Epoch=1, Valid ROC AUC=0.8367422185846374\n",
      "Epoch=2, Valid ROC AUC=0.8662285357730363\n",
      "Epoch=3, Valid ROC AUC=0.7852847985867443\n",
      "Epoch=4, Valid ROC AUC=0.8493513123667982\n",
      "Epoch=5, Valid ROC AUC=0.8461447452655122\n",
      "Epoch=6, Valid ROC AUC=0.8582262991985609\n",
      "Epoch=7, Valid ROC AUC=0.8661580350558737\n",
      "Epoch=8, Valid ROC AUC=0.864488707729958\n",
      "Epoch=9, Valid ROC AUC=0.8516429908511137\n"
     ]
    }
   ],
   "source": [
    "# train.py\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import albumentations\n",
    "import torch\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# location of train.csv and train_png folder\n",
    "# with all the png images\n",
    "data_path = \"../input/siim-png-train-csv/\"\n",
    "\n",
    "# cuda/cpu device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# let's train for 10 epoches\n",
    "epoches = 10\n",
    "\n",
    "# load the dataframe\n",
    "# https://www.kaggle.com/abhishek/train-your-own-mask-rcnn/data?select=train-rle.csv\n",
    "df = pd.read_csv(os.path.join(data_path, 'train.csv'))\n",
    "\n",
    "# fetch all image ids\n",
    "images = df.ImageId.values.tolist()\n",
    "\n",
    "# a list with image locations\n",
    "# https://www.kaggle.com/abhishek/siim-png-images\n",
    "images = [\n",
    "    os.path.join(\"../input/siim-png-images/\", 'train_png', image + '.png') for image in images\n",
    "]\n",
    "\n",
    "# binary target numpy array\n",
    "targets = df.target.values\n",
    "\n",
    "# fetch out model, we will try both pretrained\n",
    "# and non-pretrained weights\n",
    "# model = get_model(pretrained=True)\n",
    "model = get_resnet18_model(pretrained=True)\n",
    "\n",
    "# move model to device\n",
    "model.to(device)\n",
    "\n",
    "# mean and std values of RGB channels for imagenet dataset\n",
    "# we use these pre-calculated values when we use weights\n",
    "# from imagenet\n",
    "# when we do not use imagenet weights, we use the mean and\n",
    "# standard deviation values of the original dataset\n",
    "# please note that this is a separate calculation\n",
    "mean = (0.485, 0.456, 0.406)\n",
    "std = (0.229, 0.224, 0.225)\n",
    "\n",
    "# albumentation is an image augmentation library\n",
    "# that allows you to do many different types of image\n",
    "# augmentations. here, i am using only normalization\n",
    "# notice always_apply=True. we always want to apply\n",
    "# normalization\n",
    "aug = albumentations.Compose(\n",
    "    [\n",
    "        albumentations.Normalize(\n",
    "            mean, std, max_pixel_value=255.0, always_apply=True\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "# instead of using kfold, i am using train_test_split\n",
    "# with a fixed random state\n",
    "train_images, valid_images, train_targets, valid_targets = train_test_split(\n",
    "    images, targets, stratify=targets, random_state=42\n",
    ")\n",
    "\n",
    "# fetch the ClassificationDataset class\n",
    "train_dataset = ClassificationDataset(\n",
    "    image_paths=train_images,\n",
    "    targets=train_targets,\n",
    "    resize=(227, 227),\n",
    "    augmentations=aug\n",
    ")\n",
    "\n",
    "# torch dataloader creates batches of data\n",
    "# from classification dataset class\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "# same for validation\n",
    "valid_dataset = ClassificationDataset(\n",
    "    image_paths=valid_images,\n",
    "    targets=valid_targets,\n",
    "    resize=(227, 227),\n",
    "    augmentations=aug\n",
    ")\n",
    "\n",
    "valid_loader = torch.utils.data.DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=False,\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "# simple Adam optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-4)\n",
    "\n",
    "# train and print auc score for all epoches\n",
    "for epoch in range(epoches):\n",
    "    train(train_loader, model, optimizer, device=device)\n",
    "    predictions, valid_targets = evaluate(\n",
    "        valid_loader, model, device=device\n",
    "    )\n",
    "    roc_auc = metrics.roc_auc_score(valid_targets, predictions)\n",
    "    print(\n",
    "        f'Epoch={epoch}, Valid ROC AUC={roc_auc}'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "native-hayes",
   "metadata": {
    "papermill": {
     "duration": 0.013737,
     "end_time": "2021-05-29T16:02:43.296827",
     "exception": false,
     "start_time": "2021-05-29T16:02:43.283090",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "following-adventure",
   "metadata": {
    "papermill": {
     "duration": 0.013963,
     "end_time": "2021-05-29T16:02:43.324770",
     "exception": false,
     "start_time": "2021-05-29T16:02:43.310807",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1722.091371,
   "end_time": "2021-05-29T16:02:45.448025",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-05-29T15:34:03.356654",
   "version": "2.3.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "08ae8aa90709410ba5e1caf7c8d64fbf": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_64385a121d2f4b3481ee4f9f728ac504",
       "max": 46827520.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_8b3141e73cf0426290760f02fe0d3f32",
       "value": 46827520.0
      }
     },
     "18816eeca9ab4cfbbe7394645ded33d6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "64385a121d2f4b3481ee4f9f728ac504": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "64fd8f2b098341dfb480eea1b3144046": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_b2f7946fe1e640b6a5baaa305d2a3343",
        "IPY_MODEL_08ae8aa90709410ba5e1caf7c8d64fbf",
        "IPY_MODEL_aa67c29714ce4cee8b65d50f86b4a9d9"
       ],
       "layout": "IPY_MODEL_daa3ac8b6c09413cb6f04ac80fee8a35"
      }
     },
     "8b3141e73cf0426290760f02fe0d3f32": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "aa67c29714ce4cee8b65d50f86b4a9d9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_dc90f526c71649dba12bf3f2670824ca",
       "placeholder": "​",
       "style": "IPY_MODEL_18816eeca9ab4cfbbe7394645ded33d6",
       "value": " 44.7M/44.7M [00:00&lt;00:00, 70.5MB/s]"
      }
     },
     "b2f7946fe1e640b6a5baaa305d2a3343": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_fa39b8ce42ec49bc94ddd02e3a3a796c",
       "placeholder": "​",
       "style": "IPY_MODEL_f8046057b5d94ee99f1d03a58d80e69a",
       "value": "100%"
      }
     },
     "daa3ac8b6c09413cb6f04ac80fee8a35": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "dc90f526c71649dba12bf3f2670824ca": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f8046057b5d94ee99f1d03a58d80e69a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "fa39b8ce42ec49bc94ddd02e3a3a796c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
