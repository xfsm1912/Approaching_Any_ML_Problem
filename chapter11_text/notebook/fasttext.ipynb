{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fasttext.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJio_PN6r8ba",
        "outputId": "b7e301a9-6c42-4685-b465-c7ab95a36cc7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!wget http://dl.fbaipublicfiles.com/fasttext/vectors-english/crawl-300d-2M.vec.zip"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-11-13 04:01:29--  http://dl.fbaipublicfiles.com/fasttext/vectors-english/crawl-300d-2M.vec.zip\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 172.67.9.4, 104.22.75.142, 104.22.74.142, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|172.67.9.4|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1523785255 (1.4G) [application/zip]\n",
            "Saving to: ‘crawl-300d-2M.vec.zip’\n",
            "\n",
            "crawl-300d-2M.vec.z 100%[===================>]   1.42G  10.6MB/s    in 2m 17s  \n",
            "\n",
            "2020-11-13 04:03:46 (10.6 MB/s) - ‘crawl-300d-2M.vec.zip’ saved [1523785255/1523785255]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wfIPuYcWvLXP"
      },
      "source": [
        "import zipfile\n",
        "with zipfile.ZipFile('/content/crawl-300d-2M.vec.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mljHVbw4uwer"
      },
      "source": [
        "!cp /content/drive/My\\ Drive/colab/Approaching_Any_ML_Problem/chapter11_text/input/IMDB\\ Dataset.csv ."
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYZlEEegvCg1",
        "outputId": "c43110f7-7b25-4ae3-9539-870b4d6a5346",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "pwd"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDGT39NZr7za",
        "outputId": "056a44cc-031a-409b-b052-4596de862dfb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_0oH8s6rwPi",
        "outputId": "ec130d4e-5b54-4259-e286-08b9444bb856",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# fasttext.py\n",
        "\n",
        "import io\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn import linear_model\n",
        "from sklearn import metrics\n",
        "from sklearn import model_selection\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "\n",
        "def load_vectors(fname):\n",
        "    # taken from\n",
        "    fin = io.open(\n",
        "        fname,\n",
        "        'r',\n",
        "        encoding='utf-8',\n",
        "        newline='\\n',\n",
        "        errors='ignore'\n",
        "    )\n",
        "    n, d = map(int, fin.readline().split())\n",
        "    data = {}\n",
        "    for line in fin:\n",
        "        tokens = line.rstrip().split(' ')\n",
        "        data[tokens[0]] = list(map(float, tokens[1:]))\n",
        "    return data\n",
        "\n",
        "\n",
        "def sentence_to_vec(s, embedding_dict, stop_words, tokenizer):\n",
        "    \"\"\"\n",
        "    Given a sentence and other information,\n",
        "    this function returns embedding for the whole sentence\n",
        "    :param s: sentence, string\n",
        "    :param embedding_dict: dictionary word: vector\n",
        "    :param stop_words: list of step words, \n",
        "    :param tokenizer:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    # convert sentence to string and lowercase it\n",
        "    words = str(s).lower()\n",
        "\n",
        "    # tokenizer the sentence\n",
        "    words = tokenizer(words)\n",
        "\n",
        "    # remove stop word tokens\n",
        "    words = [w for w in words if not stop_words]\n",
        "\n",
        "    # keep only alpha-numeric tokens\n",
        "    words = [w for w in words if w.isalpha()]\n",
        "\n",
        "    # initialize empty list to store embeddings\n",
        "    M = []\n",
        "\n",
        "    for w in words:\n",
        "        # for every word, fetch the embedding from\n",
        "        # the dictionary and append to list of\n",
        "        # embeddings\n",
        "        if w in embedding_dict:\n",
        "            M.append(embedding_dict[w])\n",
        "\n",
        "    # if we don't have any vectors, return zeros\n",
        "    if len(M) == 0:\n",
        "        return np.zeros(300)\n",
        "\n",
        "    # convert list of embeddings to array\n",
        "    M = np.array(M)\n",
        "\n",
        "    # calculate sum over axis=0\n",
        "    v = M.sum(axis=0)\n",
        "\n",
        "    # return the normalized vector\n",
        "    return v / np.sqrt((v ** 2).sum())\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # read the training data\n",
        "    df = pd.read_csv(\"./IMDB Dataset.csv\")\n",
        "\n",
        "    # map positive to 1 and negative to 0\n",
        "    df.sentiment = df.sentiment.apply(\n",
        "        lambda x: 1 if x == \"positive\" else 0\n",
        "    )\n",
        "\n",
        "    # the next step is to randomize the rows of the data\n",
        "    df = df.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "    # load embeddings into memory\n",
        "    print(\"Loading embeddings\")\n",
        "    # https://www.kaggle.com/yekenot/fasttext-crawl-300d-2m\n",
        "    embeddings = load_vectors(\"./crawl-300d-2M.vec\")\n",
        "\n",
        "    # create sentence embeddings\n",
        "    print(\"Creating sentence vectors\")\n",
        "    vectors = []\n",
        "\n",
        "    for review in df.review.values:\n",
        "        vectors.append(\n",
        "            sentence_to_vec(\n",
        "                s=review,\n",
        "                embedding_dict=embeddings,\n",
        "                stop_words=[],\n",
        "                tokenizer=word_tokenize\n",
        "            )\n",
        "        )\n",
        "\n",
        "    vectors = np.array(vectors)\n",
        "\n",
        "    # fetch labels\n",
        "    y = df.sentiment.values\n",
        "\n",
        "    # initiate the kfold class from model_selection module\n",
        "    kf = model_selection.StratifiedKFold(n_splits=5)\n",
        "\n",
        "    # fill the new kfold column\n",
        "    for fold_, (t_, v_) in enumerate(kf.split(X=vectors, y=y)):\n",
        "        print(f\"Training fold: {fold_}\")\n",
        "        # temporary dataframes for train and test\n",
        "        xtrain = vectors[t_, :]\n",
        "        ytrain = y[t_]\n",
        "\n",
        "        xtest = vectors[v_, :]\n",
        "        ytest = y[v_]\n",
        "\n",
        "        # initialize logistic regression model\n",
        "        model = linear_model.LogisticRegression()\n",
        "\n",
        "        # fit the model on training data reviews and sentiment\n",
        "        model.fit(xtrain, ytrain)\n",
        "\n",
        "        # make predictions on test data\n",
        "        # threshold for predictions is 0.5\n",
        "        preds = model.predict(xtest)\n",
        "\n",
        "        # calculation accuracy\n",
        "        accuracy = metrics.accuracy_score(ytest, preds)\n",
        "\n",
        "        print(f\"Accuracy = {accuracy}\")\n",
        "        print(\"\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading embeddings\n",
            "Creating sentence vectors\n",
            "Training fold: 0\n",
            "Accuracy = 0.8646\n",
            "\n",
            "Training fold: 1\n",
            "Accuracy = 0.8576\n",
            "\n",
            "Training fold: 2\n",
            "Accuracy = 0.8624\n",
            "\n",
            "Training fold: 3\n",
            "Accuracy = 0.8585\n",
            "\n",
            "Training fold: 4\n",
            "Accuracy = 0.8586\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NY7FX_k1yZ_m"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}