{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"train.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","mount_file_id":"1y3x7crBya4eO6RFeKe53QT119DWSZgWp","authorship_tag":"ABX9TyMVTI+1y0RmDQYpmBsDWBj5"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"GG4csQ7Gi8xM","executionInfo":{"status":"ok","timestamp":1605421000397,"user_tz":300,"elapsed":37021,"user":{"displayName":"Jianhua Ma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjddOQhZ0WuOelaLCJ3rGZp5NxXHlzzJgdH9WA3=s64","userId":"09332100007424911325"}},"outputId":"6a664ce2-b835-4537-8c2f-d7db9ed7a0b3","colab":{"base_uri":"https://localhost:8080/"}},"source":["!wget http://dl.fbaipublicfiles.com/fasttext/vectors-english/crawl-300d-2M.vec.zip"],"execution_count":1,"outputs":[{"output_type":"stream","text":["--2020-11-15 06:16:04--  http://dl.fbaipublicfiles.com/fasttext/vectors-english/crawl-300d-2M.vec.zip\n","Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.22.75.142, 172.67.9.4, 104.22.74.142, ...\n","Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.22.75.142|:80... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 1523785255 (1.4G) [application/zip]\n","Saving to: ‘crawl-300d-2M.vec.zip’\n","\n","crawl-300d-2M.vec.z 100%[===================>]   1.42G  35.8MB/s    in 34s     \n","\n","2020-11-15 06:16:39 (42.3 MB/s) - ‘crawl-300d-2M.vec.zip’ saved [1523785255/1523785255]\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"pA4JGoGGjAiR","executionInfo":{"status":"ok","timestamp":1605421039978,"user_tz":300,"elapsed":60165,"user":{"displayName":"Jianhua Ma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjddOQhZ0WuOelaLCJ3rGZp5NxXHlzzJgdH9WA3=s64","userId":"09332100007424911325"}}},"source":["import zipfile\n","with zipfile.ZipFile('/content/crawl-300d-2M.vec.zip', 'r') as zip_ref:\n","    zip_ref.extractall('/content/')"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"Dh1qZCThjDqx","executionInfo":{"status":"ok","timestamp":1605421057165,"user_tz":300,"elapsed":66387,"user":{"displayName":"Jianhua Ma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjddOQhZ0WuOelaLCJ3rGZp5NxXHlzzJgdH9WA3=s64","userId":"09332100007424911325"}}},"source":["!cp /content/drive/My\\ Drive/colab/Approaching_Any_ML_Problem/chapter11_text/input/IMDB\\ Dataset.csv ."],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"y-mA4AI1jEjD","executionInfo":{"status":"ok","timestamp":1605421059324,"user_tz":300,"elapsed":49498,"user":{"displayName":"Jianhua Ma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjddOQhZ0WuOelaLCJ3rGZp5NxXHlzzJgdH9WA3=s64","userId":"09332100007424911325"}},"outputId":"3575aff5-a715-42ed-bc05-6361c0cb4306","colab":{"base_uri":"https://localhost:8080/"}},"source":["import nltk\n","nltk.download('punkt')"],"execution_count":4,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"id":"_ZICBanzjgVi","executionInfo":{"status":"ok","timestamp":1605421118116,"user_tz":300,"elapsed":2320,"user":{"displayName":"Jianhua Ma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjddOQhZ0WuOelaLCJ3rGZp5NxXHlzzJgdH9WA3=s64","userId":"09332100007424911325"}}},"source":["# create_folds.py\n","# import pandas and model_selection module of scikit-learn\n","import pandas as pd\n","from sklearn import model_selection\n","\n","if __name__ == \"__main__\":\n","    # Read training data\n","    df = pd.read_csv(\"./IMDB Dataset.csv\")\n","\n","    # map positive to 1 and negative to 0\n","    df.sentiment = df.sentiment.apply(\n","        lambda x: 1 if x == \"positive\" else 0\n","    )\n","\n","    # we create a new column called kfold and fill it with -1\n","    df[\"kfold\"] = -1\n","\n","    # the next step is to randomize the rows of the data\n","    df = df.sample(frac=1).reset_index(drop=True)\n","\n","    # fetch labels\n","    y = df.sentiment.values\n","\n","    # initiate the kfold class from model_selection module\n","    kf = model_selection.StratifiedKFold(n_splits=5)\n","\n","    # fill the new kfold column\n","    for f, (t_, v_) in enumerate(kf.split(X=df, y=y)):\n","        df.loc[v_, 'kfold'] = f\n","\n","    # save the new csv with kfold column\n","    df.to_csv(\"./imdb_folds.csv\", index=False)\n"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"E2BHoLHbjwc8","executionInfo":{"status":"ok","timestamp":1605421246915,"user_tz":300,"elapsed":1983,"user":{"displayName":"Jianhua Ma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjddOQhZ0WuOelaLCJ3rGZp5NxXHlzzJgdH9WA3=s64","userId":"09332100007424911325"}}},"source":["!cp /content/drive/My\\ Drive/colab/Approaching_Any_ML_Problem/chapter11_text/config.py .\n","!cp /content/drive/My\\ Drive/colab/Approaching_Any_ML_Problem/chapter11_text/dataset.py .\n","!cp /content/drive/My\\ Drive/colab/Approaching_Any_ML_Problem/chapter11_text/engine.py .\n","!cp /content/drive/My\\ Drive/colab/Approaching_Any_ML_Problem/chapter11_text/lstm.py ."],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"g13-v8iXijtY","executionInfo":{"status":"ok","timestamp":1605424816173,"user_tz":300,"elapsed":3472950,"user":{"displayName":"Jianhua Ma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjddOQhZ0WuOelaLCJ3rGZp5NxXHlzzJgdH9WA3=s64","userId":"09332100007424911325"}},"outputId":"869c24d6-f6e2-45c6-e988-ea509915aa0c","colab":{"base_uri":"https://localhost:8080/"}},"source":["# train.py\n","import io\n","import torch\n","\n","import numpy as np\n","import pandas as pd\n","\n","# yes, we use tensorflow\n","# but not for training the model !\n","\n","import tensorflow as tf\n","\n","from sklearn import metrics\n","import torch.utils.data.dataloader\n","\n","import config as config\n","import dataset as dataset\n","import engine as engine\n","import lstm as lstm\n","\n","\n","def load_vectors(fname):\n","    # taken from: https://fasttext.cc/docs/en/english-vectors.html\n","    fin = io.open(\n","        fname,\n","        'r',\n","        encoding='utf-8',\n","        newline='\\n',\n","        errors='ignore'\n","    )\n","    n, d = map(int, fin.readline().split())\n","\n","    data = {}\n","    for line in fin:\n","        tokens = line.rstrip().split(' ')\n","        data[tokens[0]] = list(map(float, tokens[1:]))\n","\n","    return data\n","\n","\n","def create_embedding_matrix(word_index, embedding_dict):\n","    \"\"\"\n","    This function creates embedding matrix.\n","    :param word_index: a dictionary with word:index_value\n","    :param embedding_dict: a dictionary with word:embedding_vector\n","    :return: a numpy array with embedding vectors for all known words\n","    \"\"\"\n","    # initialize matrix with zeros\n","    embedding_matrix = np.zeros((len(word_index) + 1, 300))\n","    # loop over all the words\n","    for word, i in word_index.items():\n","        # if word is found in pre-trained embeddings,\n","        # update the matrix. if the word is not found,\n","        # the vector is zeor!\n","        if word in embedding_dict:\n","            embedding_matrix[i] = embedding_dict[word]\n","\n","    # return embedding matrix\n","    return embedding_matrix\n","\n","\n","def run(df, fold):\n","    \"\"\"\n","    Run training and validation for a given fold and dataset\n","    :param df: pandas dataframe with kfold column\n","    :param fold: current fold, int\n","    :return:\n","    \"\"\"\n","    # fetch training dataframe\n","    train_df = df[df.kfold != fold].reset_index(drop=True)\n","\n","    # fetch validation dataframe\n","    valid_df = df[df.kfold == fold].reset_index(drop=True)\n","\n","    print(\"Fitting tokenizer\")\n","    # we use tf.keras for tokenization\n","    # you can use your own tokenizer and then you can get rid of tensorflow\n","    tokenizer = tf.keras.preprocessing.text.Tokenizer()\n","    tokenizer.fit_on_texts(df.review.values.tolist())\n","\n","    # convert training data to sequences\n","    # for example: \"bad movie\" gets converted to\n","    # [24, 27] where 24 is the index for bad and 27 is the\n","    # index for movie\n","    xtrain = tokenizer.texts_to_sequences(train_df.review.values)\n","\n","    # similarly convert validation data to\n","    # sequences\n","    xtest = tokenizer.texts_to_sequences(valid_df.review.values)\n","\n","    # zero pad the training sequences given the maximum length\n","    # this padding is done on left hand side\n","    # if sequence is > MAX_LEN, it is truncated on left hand side too\n","    xtrain = tf.keras.preprocessing.sequence.pad_sequences(\n","        xtrain, maxlen=config.MAX_LEN\n","    )\n","\n","    # zero pad the validation sequences\n","    xtest = tf.keras.preprocessing.sequence.pad_sequences(\n","        xtest, maxlen=config.MAX_LEN\n","    )\n","\n","    # initialize dataset class for training\n","    train_dataset = dataset.IMDBDataset(\n","        reviews=xtrain,\n","        targets=train_df.sentiment.values\n","    )\n","\n","    # create torch dataloader for training\n","    # torch dataloader loads the data using dataset\n","    # class in batches specified by batch size\n","    train_data_loader = torch.utils.data.dataloader.DataLoader(\n","        dataset=train_dataset,\n","        batch_size=config.TRAIN_BATCH_SIZE,\n","        num_workers=2\n","    )\n","\n","    # initialize dataset class for validation\n","    valid_dataset = dataset.IMDBDataset(\n","        reviews=xtest,\n","        targets=valid_df.sentiment.values\n","    )\n","\n","    # create torch dataloader for validation\n","    valid_data_loader = torch.utils.data.dataloader.DataLoader(\n","        dataset=valid_dataset,\n","        batch_size=config.VALID_BATCH_SIZE,\n","        num_workers=1\n","    )\n","\n","    print(\"Loading embeddings\")\n","    # load embeddings as shown previously\n","    embedding_dict = load_vectors(\"./crawl-300d-2M.vec\")\n","    embedding_matrix = create_embedding_matrix(\n","        tokenizer.word_index, embedding_dict\n","    )\n","\n","    # create torch device, since we use gpu, we are using cuda\n","    device = torch.device(\"cuda\")\n","\n","    # fetch our LSTM model\n","    model = lstm.LSTM(embedding_matrix)\n","\n","    # send model to device\n","    model.to(device)\n","\n","    # initialize Adam optimizer\n","    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n","\n","    print(\"Training Model\")\n","    # set best accuracy to zero\n","    best_accuracy = 0\n","    # set early stopping counter to zero\n","    early_stopping_counter = 0\n","    # train and validate for all epoches\n","    for epoch in range(config.EPOCHS):\n","        # train one epoch\n","        engine.train(train_data_loader, model, optimizer, device)\n","        # validate\n","        outputs, targets = engine.evaluate(\n","            valid_data_loader, model, device\n","        )\n","\n","        # use threshold of 0.5\n","        # please note we are using linear layer and no sigmoid\n","        # you should do this 0.5 threshold after sigmoid\n","        outputs = np.array(outputs) >= 0.5\n","\n","        # calculate accuracy\n","        accuracy = metrics.accuracy_score(targets, outputs)\n","        print(\n","            f\"FOLD:{fold}, Epoch: {epoch}, Accuracy Score = {accuracy}\"\n","        )\n","        # simple early stopping\n","        if accuracy > best_accuracy:\n","            best_accuracy = accuracy\n","        else:\n","            early_stopping_counter += 1\n","\n","        if early_stopping_counter > 2:\n","            break\n","\n","\n","if __name__ == \"__main__\":\n","\n","    # load data\n","    df = pd.read_csv(\"./imdb_folds.csv\")\n","\n","    # train for all folds\n","    run(df, fold=0)\n","    run(df, fold=1)\n","    run(df, fold=2)\n","    run(df, fold=3)\n","    run(df, fold=4)\n"],"execution_count":10,"outputs":[{"output_type":"stream","text":["Fitting tokenizer\n","Loading embeddings\n","Training Model\n","FOLD:0, Epoch: 0, Accuracy Score = 0.8843\n","FOLD:0, Epoch: 1, Accuracy Score = 0.8919\n","FOLD:0, Epoch: 2, Accuracy Score = 0.9005\n","FOLD:0, Epoch: 3, Accuracy Score = 0.9008\n","FOLD:0, Epoch: 4, Accuracy Score = 0.8933\n","FOLD:0, Epoch: 5, Accuracy Score = 0.886\n","FOLD:0, Epoch: 6, Accuracy Score = 0.8876\n","Fitting tokenizer\n","Loading embeddings\n","Training Model\n","FOLD:1, Epoch: 0, Accuracy Score = 0.8831\n","FOLD:1, Epoch: 1, Accuracy Score = 0.8998\n","FOLD:1, Epoch: 2, Accuracy Score = 0.903\n","FOLD:1, Epoch: 3, Accuracy Score = 0.9058\n","FOLD:1, Epoch: 4, Accuracy Score = 0.9033\n","FOLD:1, Epoch: 5, Accuracy Score = 0.9023\n","FOLD:1, Epoch: 6, Accuracy Score = 0.9025\n","Fitting tokenizer\n","Loading embeddings\n","Training Model\n","FOLD:2, Epoch: 0, Accuracy Score = 0.8732\n","FOLD:2, Epoch: 1, Accuracy Score = 0.8922\n","FOLD:2, Epoch: 2, Accuracy Score = 0.8996\n","FOLD:2, Epoch: 3, Accuracy Score = 0.8976\n","FOLD:2, Epoch: 4, Accuracy Score = 0.9009\n","FOLD:2, Epoch: 5, Accuracy Score = 0.9002\n","FOLD:2, Epoch: 6, Accuracy Score = 0.885\n","Fitting tokenizer\n","Loading embeddings\n","Training Model\n","FOLD:3, Epoch: 0, Accuracy Score = 0.8786\n","FOLD:3, Epoch: 1, Accuracy Score = 0.8937\n","FOLD:3, Epoch: 2, Accuracy Score = 0.8974\n","FOLD:3, Epoch: 3, Accuracy Score = 0.898\n","FOLD:3, Epoch: 4, Accuracy Score = 0.896\n","FOLD:3, Epoch: 5, Accuracy Score = 0.8901\n","FOLD:3, Epoch: 6, Accuracy Score = 0.8887\n","Fitting tokenizer\n","Loading embeddings\n","Training Model\n","FOLD:4, Epoch: 0, Accuracy Score = 0.8809\n","FOLD:4, Epoch: 1, Accuracy Score = 0.8892\n","FOLD:4, Epoch: 2, Accuracy Score = 0.8915\n","FOLD:4, Epoch: 3, Accuracy Score = 0.8928\n","FOLD:4, Epoch: 4, Accuracy Score = 0.8907\n","FOLD:4, Epoch: 5, Accuracy Score = 0.89\n","FOLD:4, Epoch: 6, Accuracy Score = 0.8947\n","FOLD:4, Epoch: 7, Accuracy Score = 0.8904\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"nyP4pCxAjmOb"},"source":[""],"execution_count":null,"outputs":[]}]}